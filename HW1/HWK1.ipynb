{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4139cc19-dd23-4ffa-b6a2-a1b70a6daec6",
   "metadata": {},
   "source": [
    "<center><h1>CSCI 4140: Natural Language Processing</h1></center>\n",
    "<center><h1>CSCI/DASC 6040: Computational Analysis of Natural Languages</h1></center>\n",
    "\n",
    "<center><h6>Spring 2024</h6></center>\n",
    "<center><h6>Homework 1 - N-gram models</h6></center>\n",
    "<center><h6>Due Sunday, January 21, at 11:59 PM</h6></center>\n",
    "\n",
    "<center><font color='red'>Do not redistribute without the instructorâ€™s written permission.</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d5179-89ef-4186-9af3-ce63ff95cc8b",
   "metadata": {},
   "source": [
    "The learning goals of this assignment are to:\n",
    "\n",
    "- Understand how to compute language model probabilities using maximum likelihood estimation.\n",
    "- Implement back-off.\n",
    "- Have fun using a language model to probabilistically generate texts.\n",
    "- Compare word-level langauage models and character-level language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40408d08-1f30-4286-be61-ac741db46121",
   "metadata": {},
   "source": [
    "# Part 1: N-gram Language model (60 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f41ccf-d54b-4f5d-bd2d-4c8811cd84d1",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84910e85-68eb-41ab-996f-65cad5d8071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02bac8f-9980-4905-8029-1d48434008cd",
   "metadata": {},
   "source": [
    "We'll start by loading the data. The WikiText language modeling dataset is a collection of tokens extracted from the set of verified Good and Featured articles on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22459dd5-ddf4-4e53-9e4a-570646a15820",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'test': '', 'train': '', 'valid': ''}\n",
    "\n",
    "for data_split in data:\n",
    "    fname = \"wiki.{}.tokens\".format(data_split)\n",
    "    with open(fname, 'r') as f_wiki:\n",
    "        data[data_split] = f_wiki.read().lower().split()\n",
    "\n",
    "vocab = list(set(data['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de2aac-d6c2-4ec2-b0d1-5e2305949705",
   "metadata": {},
   "source": [
    "Now have a look at the data by running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d733449a-e810-4cd1-a03d-e309d41e1dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train : %s ...' % data['train'][:10])\n",
    "print('dev : %s ...' % data['valid'][:10])\n",
    "print('test : %s ...' % data['test'][:10])\n",
    "print('first 10 words in vocab: %s' % vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e25cc-9b86-408a-b6f0-1fb139effc34",
   "metadata": {},
   "source": [
    "## Q1.1: Train N-gram language model (25 pts)\n",
    "\n",
    "Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n",
    "\n",
    "*Input:*\n",
    "+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n",
    "+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(w_2 | w_0, w_1)$.\n",
    "\n",
    "*Output:*\n",
    "+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next word computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(w_3 | w_0,w_1,w_2)$ as well as $p(w_3|w_1,w_2)$ and $p(w_3|w_2)$. \n",
    "\n",
    "Each key should be a single string where the words that form the history have been concatenated using spaces. Given a key, its corresponding value should be a dictionary where each word type in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'w1 w2' should look like:\n",
    "\n",
    "    \n",
    "    lm['w1 w2'] = {'w0': 0.001, 'w1' : 1e-6, 'w2' : 1e-6, 'w3': 0.003, ...}\n",
    "    \n",
    "In this example, we also want to store `lm['w2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n",
    "\n",
    "*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19adcd85-9e8e-4aae-b32b-05029a953dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_lm(data, order=3):\n",
    "    \"\"\"\n",
    "        Train n-gram language model\n",
    "    \"\"\"\n",
    "    \n",
    "    # pad (order-1) special tokens to the left\n",
    "    # for the first token in the text\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data # \n",
    "    lm = defaultdict(Counter)\n",
    "    for i in range(len(data) - order):\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035c635-bddb-480b-940a-fc38a0482b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ngram_lm():\n",
    "  \n",
    "    print('checking empty history ...')\n",
    "    lm1 = train_ngram_lm(data['train'], order=1)\n",
    "    assert '' in lm1, \"empty history should be in the language model!\"\n",
    "    \n",
    "    print('checking probability distributions ...')\n",
    "    lm2 = train_ngram_lm(data['train'], order=2)\n",
    "    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n",
    "    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][word] should sum to 1!\"\n",
    "    \n",
    "    print('checking lengths of histories ...')\n",
    "    lm3 = train_ngram_lm(data['train'], order=3)\n",
    "    assert len(set([len(k.split()) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n",
    "    \n",
    "    print('checking word distribution values ...')\n",
    "    assert lm1['']['the'] < 0.064 and lm1['']['the'] > 0.062 and \\\n",
    "           lm2['the']['first'] < 0.017 and lm2['the']['first'] > 0.016 and \\\n",
    "           lm3['the first']['time'] < 0.106 and lm3['the first']['time'] > 0.105, \\\n",
    "           \"values do not match!\"\n",
    "    \n",
    "    print(\"Congratulations, you passed the ngram check!\")\n",
    "    \n",
    "test_ngram_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c298d5-e036-4a1e-953c-a3888336b8bc",
   "metadata": {},
   "source": [
    "## Q1.2: Generate text from n-gram language model (10 pts)\n",
    "\n",
    "Complete the following `generate_text` function based on these input/output requirements:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n",
    "+ **vocab**: vocab is a list of unique word types in the training set, already computed for you during data loading.\n",
    "+ **context**: the input context string that you want to condition your language model on, should be a space-separated string of tokens\n",
    "+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n",
    "+ **num_tok**: number of tokens to be generated following the input context\n",
    "\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ generated text, should be a space-separated string\n",
    "    \n",
    "*Hint:*\n",
    "\n",
    "After getting the next-word distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next word from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9833b768-b01c-4f4e-9bca-70019f18b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text\n",
    "def generate_text(lm, vocab, context=\"he is the\", order=3, num_tok=25):\n",
    "    \n",
    "    # The goal is to generate new words following the context\n",
    "    # If context has more tokens than the order of lm, \n",
    "    # generate text that follows the last (order-1) tokens of the context\n",
    "    # and store it in the variable `history`\n",
    "    order -= 1\n",
    "    history = context.split()[-order:]\n",
    "    # `out` is the list of tokens of context\n",
    "    # you need to append the generated tokens to this list\n",
    "    out = context.split()\n",
    "        \n",
    "    for i in range(num_tok):\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbab80-83be-4789-833e-ad77eb8b06d5",
   "metadata": {},
   "source": [
    "Now try to generate some texts, generated by ngram language model with different orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4f974-09f0-4fea-9067-7dba785dc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 1\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21de76f-4864-439d-97a1-d78d250d7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 2\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac1484f-00bb-4519-9134-5b447b68b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 3\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ee800-b881-4173-8c95-525213b6136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe32855-37be-494e-bb66-b96bdc6b5f5d",
   "metadata": {},
   "source": [
    "## Q1.3 : Evaluate the models (25 pts)\n",
    "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n",
    "\n",
    "Recall perplexity is the inverse probability of the test text\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n",
    "\n",
    "For an n-gram model, perplexity is computed by\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n",
    "\n",
    "To address the numerical issue (underflow), we usually compute\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n",
    "\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n",
    "+ **data**: test data\n",
    "+ **vocab**: the list of unique word types in the training set\n",
    "+ **order**: order of the lm\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ the perplexity of test data\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e412a7-2ad8-4a79-a4b5-31bc81295e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "def compute_perplexity(lm, data, vocab, order=3):\n",
    "    \n",
    "    # pad according to order\n",
    "    order -= 1\n",
    "    data = ['<S>'] * order + data\n",
    "    log_sum = 0\n",
    "    for i in range(len(data) - order):\n",
    "        h, w = ' '.join(data[i: i+order]), data[i+order]\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        # if h not in lm, back-off to n-1 gram and look up again\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02749daa-96b5-4f50-aa1c-0279a6e24cef",
   "metadata": {},
   "source": [
    "Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 795, 203, 141, and 130 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5293167-78a8-49c6-93dd-cbfdff8c3212",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in [1, 2, 3, 4]:\n",
    "    lm = train_ngram_lm(data['train'], order=o)\n",
    "    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0559387-d8e1-4415-875d-e4157a47f3c5",
   "metadata": {},
   "source": [
    "# Part 2: Character-level N-gram language model (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc0480-756c-4de2-b675-bca705c8f475",
   "metadata": {},
   "source": [
    "In the lecture, language modeling was defined as the task of predicting the next word in a sequence given the previous words. In this part of the assignment, we will focus on the related problem of predicting the next character or word in a sequence given the previous characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0b20a-6d03-441b-8f58-72fb6f1fb751",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80042540-c566-4933-a5bc-323f5b2ea248",
   "metadata": {},
   "source": [
    "We have to modify how we load the data, by splitting it into characters rather than words. Also, we'll use both upper case and lower case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64ffff-e278-4dcf-84d7-9ee11f28580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'test': '', 'train': '', 'valid': ''}\n",
    "\n",
    "for data_split in data:\n",
    "    fname = \"wiki.{}.tokens\".format(data_split)\n",
    "    data[data_split] = list(open(fname, 'r', encoding = 'utf-8').read())\n",
    "\n",
    "vocab = list(set(data['train']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed614e16-882a-4393-b9af-d3bc7a3f8cba",
   "metadata": {},
   "source": [
    "Now have a look at the data by running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64409d7-4e54-43ef-81c1-2f3227bb390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train : %s ...' % data['train'][:10])\n",
    "print('dev : %s ...' % data['valid'][:10])\n",
    "print('test : %s ...' % data['test'][:10])\n",
    "print('first 10 characters in vocab: %s' % vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c252f-6345-4d6e-9c88-e2a17d790bf0",
   "metadata": {},
   "source": [
    "## Q2.1: Train N-gram language model (20 pts)\n",
    "\n",
    "Complete the following `train_ngram_lm` function based on the following input/output specifications. If you've done it right, you should pass the tests in the cell below.\n",
    "\n",
    "*Input:*\n",
    "+ **data**: the data object created in the cell above that holds the tokenized Wikitext data\n",
    "+ **order**: the order of the model (i.e., the \"n\" in \"n-gram\" model). If order=3, we compute $p(c_2 | c_0, c_1)$.\n",
    "\n",
    "*Output:*\n",
    "+ **lm**: A dictionary where the key is the history and the value is a probability distribution over the next character computed using the maximum likelihood estimate from the training data. Importantly, this dictionary should include *backoff* probabilities as well; e.g., for order=4, we want to store $p(c_3 | c_0,c_1,c_2)$ as well as $p(c_3|c_1,c_2)$ and $p(c_3|c_2)$. \n",
    "\n",
    "Each key should be a single string where the characters that form the history have been concatenated. Given a key, its corresponding value should be a dictionary where each character in the vocabulary is associated with its probability of appearing after the key. For example, the entry for the history 'c1c2' should look like:\n",
    "\n",
    "    \n",
    "    lm['c1c2'] = {'c0': 0.001, 'c1' : 1e-6, 'c2' : 1e-6, 'c3': 0.003, ...}\n",
    "    \n",
    "In this example, we also want to store `lm['c2']` and `lm['']`, which contain the bigram and unigram distributions respectively.\n",
    "\n",
    "*Hint:* You might find the **defaultdict** and **Counter** classes in the **collections** module to be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7955f9-38f6-4205-b440-78f458186768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ngram_lm(data, order=3):\n",
    "    \"\"\"\n",
    "        Train n-gram language model\n",
    "    \"\"\"\n",
    "    \n",
    "    # pad (order-1) special tokens to the left\n",
    "    # for the first token in the text\n",
    "    order -= 1\n",
    "    data = ['~'] * order + data # \n",
    "    lm = defaultdict(Counter)\n",
    "    for i in range(len(data) - order):\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f5c48-8efc-4806-996c-17b1230edc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ngram_lm():\n",
    "  \n",
    "    print('checking empty history ...')\n",
    "    lm1 = train_ngram_lm(data['train'], order=1)\n",
    "    assert '' in lm1, \"empty history should be in the language model!\"\n",
    "    \n",
    "    print('checking probability distributions ...')\n",
    "    lm2 = train_ngram_lm(data['train'], order=2)\n",
    "    sample = [sum(lm2[k].values()) for k in random.sample(list(lm2), 10)]\n",
    "    assert all([a > 0.999 and a < 1.001 for a in sample]), \"lm[history][character] should sum to 1!\"\n",
    "    \n",
    "    print('checking lengths of histories ...')\n",
    "    lm3 = train_ngram_lm(data['train'], order=3)\n",
    "    assert len(set([len(k) for k in list(lm3)])) == 3, \"lm object should store histories of all sizes!\"\n",
    "    \n",
    "    print('checking character distribution values ...')\n",
    "    assert lm1['']['t'] < 0.062 and lm1['']['t'] > 0.060 and \\\n",
    "           lm2['t']['h'] < 0.297 and lm2['t']['h'] > 0.296 and \\\n",
    "           lm3['th']['e'] < 0.694 and lm3['th']['e'] > 0.693, \\\n",
    "           \"values do not match!\"\n",
    "    \n",
    "    print(\"Congratulations, you passed the ngram check!\")\n",
    "    \n",
    "test_ngram_lm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1bfa82-d114-4be7-9c60-e86044c3da95",
   "metadata": {},
   "source": [
    "## Q2.2: Generate text from n-gram language model (10 pts)\n",
    "\n",
    "Complete the following `generate_text` function based on these input/output requirements:\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the lm object is the dictionary you return from  the **train_ngram_lm** function\n",
    "+ **vocab**: vocab is a list of unique characters in the training set, already computed for you during data loading.\n",
    "+ **context**: the input context string that you want to condition your language model on, should be a string\n",
    "+ **order**: order of your language model (i.e., \"n\" in the \"n-gram\" model)\n",
    "+ **num_tok**: number of characters to be generated following the input context\n",
    "\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ generated text, should be a sequence of characters\n",
    "    \n",
    "*Hint:*\n",
    "\n",
    "After getting the next-character distribution given history, try using **[numpy.random.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)** to sample the next character from the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa2689-f634-49b0-87aa-40d870fa98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text\n",
    "def generate_text(lm, vocab, context=\"he \", order=3, num_tok=25):\n",
    "    \n",
    "    # The goal is to generate new characters following the context\n",
    "    # If context has more tokens than the order of lm, \n",
    "    # generate text that follows the last (order-1) tokens of the context\n",
    "    # and store it in the variable `history`\n",
    "    order -= 1\n",
    "    history = list(context)[-order:]\n",
    "    # `out` is the list of tokens of context\n",
    "    # you need to append the generated tokens to this list\n",
    "    out = list(context)\n",
    "        \n",
    "    for i in range(num_tok):\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8f64b-3bcc-4d54-bc10-8dde3b606c14",
   "metadata": {},
   "source": [
    "Now try to generate some texts, generated by ngram language model with different orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a210e-84ff-49cd-b408-2bea13ae1fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 1\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783bb02-15e8-4e52-bae3-7488d877f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 2\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672f49d-c6fa-4ed1-8090-5733cc56eb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 3\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca5823-8440-417f-aa23-3a54d0000d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 4\n",
    "generate_text(train_ngram_lm(data['train'], order=order), vocab, context='he is the', order=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9dd7c5-d2da-489b-8048-cae71cd8a98e",
   "metadata": {},
   "source": [
    "## Q2.3 : Evaluate the models (20 pts)\n",
    "Now let's evaluate the models quantitively using the intrinsic metric **perplexity**. \n",
    "\n",
    "Recall perplexity is the inverse probability of the test text\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = P(w_1, \\dots, w_t)^{-\\frac{1}{T}}$$\n",
    "\n",
    "For an n-gram model, perplexity is computed by\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\left[\\prod_{t=1}^T P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right]^{-\\frac{1}{T}}$$\n",
    "\n",
    "To address the numerical issue (underflow), we usually compute\n",
    "$$\\text{PP}(w_1, \\dots, w_t) = \\exp\\left(-\\frac{1}{T}\\sum_i \\log P(w_t|w_{t-1},\\ldots,w_{t-n+1})\\right)$$\n",
    "\n",
    "\n",
    "*Input:*\n",
    "\n",
    "+ **lm**: the language model you trained (the object you returned from the `train_ngram_lm` function)\n",
    "+ **data**: test data\n",
    "+ **vocab**: the list of unique characters in the training set\n",
    "+ **order**: order of the lm\n",
    "\n",
    "*Output:*\n",
    "\n",
    "+ the perplexity of test data\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "+ If the history is not in the **lm** object, back-off to (n-1) order history to check if it is in **lm**. If no history can be found, just use `1/|V|` where `|V|` is the size of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bf0607-be63-4e4e-8e98-10ae38c204fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, exp\n",
    "def compute_perplexity(lm, data, vocab, order=3):\n",
    "    \n",
    "    # pad according to order\n",
    "    order -= 1\n",
    "    data = ['~'] * order + data\n",
    "    log_sum = 0\n",
    "    for i in range(len(data) - order):\n",
    "        h, w = ''.join(data[i: i+order]), data[i+order]\n",
    "        \"\"\"\n",
    "        IMPLEMENT ME!\n",
    "        # if h not in lm, back-off to n-1 gram and look up again\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e9830-af0d-4a9e-ab3d-2639155a2279",
   "metadata": {},
   "source": [
    "Let's evaluate the language model with different orders. You should see a decrease in perplexity as the order increases. As a reference, the perplexity of the unigram, bigram, trigram, and 4-gram language models should be around 24.5, 10.4, 6.5, and 4.5 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da1177-f2dd-4ad4-8192-bcd622771257",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in [1, 2, 3, 4]:\n",
    "    lm = train_ngram_lm(data['train'], order=o)\n",
    "    print('order {} ppl {}'.format(o, compute_perplexity(lm, data['test'], vocab, order=o)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
